{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Get Started with TVM\n",
    "====================\n",
    "**Author**: `Tianqi Chen <https://tqchen.github.io>`_\n",
    "\n",
    "This is an introduction tutorial to TVM.\n",
    "TVM is a domain specific language for efficient kernel construction.\n",
    "\n",
    "In this tutorial, we will demonstrate the basic workflow in TVM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import tvm\n",
    "import numpy as np\n",
    "\n",
    "# Global declarations of environment.\n",
    "\n",
    "tgt_host=\"llvm\"\n",
    "# Change it to respective GPU if gpu is enabled Ex: cuda, opencl\n",
    "tgt=\"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Add Example\n",
    "------------------\n",
    "In this tutorial, we will use a vector addition example to demonstrate\n",
    "the workflow.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the Computation\n",
    "------------------------\n",
    "As a first step, we need to describe our computation.\n",
    "TVM adopts tensor semantics, with each intermediate result\n",
    "represented as multi-dimensional array. The user need to describe\n",
    "the computation rule that generate the tensors.\n",
    "\n",
    "We first define a symbolic variable n to represent the shape.\n",
    "We then define two placeholder Tensors, A and B, with given shape (n,)\n",
    "\n",
    "We then describe the result tensor C, with a compute operation.\n",
    "The compute function takes the shape of the tensor, as well as a lambda function\n",
    "that describes the computation rule for each position of the tensor.\n",
    "\n",
    "No computation happens during this phase, as we are only declaring how\n",
    "the computation should be done.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tvm.tensor.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "n = tvm.var(\"n\")\n",
    "A = tvm.placeholder((n,), name='A')\n",
    "B = tvm.placeholder((n,), name='B')\n",
    "C = tvm.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\n",
    "print(type(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedule the Computation\n",
    "------------------------\n",
    "While the above lines describes the computation rule, we can compute\n",
    "C in many ways since the axis of C can be computed in data parallel manner.\n",
    "TVM asks user to provide a description of computation called schedule.\n",
    "\n",
    "A schedule is a set of transformation of computation that transforms\n",
    "the loop of computations in the program.\n",
    "\n",
    "After we construct the schedule, by default the schedule computes\n",
    "C in a serial manner in a row-major order.\n",
    "\n",
    ".. code-block:: c\n",
    "\n",
    "  for (int i = 0; i < n; ++i) {\n",
    "    C[i] = A[i] + B[i];\n",
    "  }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tvm.create_schedule(C.op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// attr [C] storage_scope = \"global\"\n",
      "allocate C[float32 * n]\n",
      "produce C {\n",
      "  for (i, 0, n) {\n",
      "    C[i] = (A[i] + B[i])\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(s, [A, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage(A, 0x3340570)\n"
     ]
    }
   ],
   "source": [
    "stageA,stageB,stageC = s.stages\n",
    "print (stageA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder(A, 0x3339050) placeholder(B, 0x333d2f0) compute(C, 0x333f610)\n"
     ]
    }
   ],
   "source": [
    "print (stageA.op, stageB.op, stageC.op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(shape=[n], op.name=A), Tensor(shape=[n], op.name=B)]\n",
      "[iter_var(i, Range(min=0, extent=n))]\n",
      "[(A(i) + B(i))]\n"
     ]
    }
   ],
   "source": [
    "print (stageC.op.input_tensors)\n",
    "print (stageC.op.axis)\n",
    "print (stageC.op.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the split construct to split the first axis of C,\n",
    "this will split the original iteration axis into product of\n",
    "two iterations. This is equivalent to the following code.\n",
    "\n",
    ".. code-block:: c\n",
    "\n",
    "  for (int bx = 0; bx < ceil(n / 64); ++bx) {\n",
    "    for (int tx = 0; tx < 64; ++tx) {\n",
    "      int i = bx * 64 + tx;\n",
    "      if (i < n) {\n",
    "        C[i] = A[i] + B[i];\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "[07:03:18] /usr/tvm/src/schedule/schedule_lang.cc:30: Operate on iter var iter_var(i, Range(min=0, extent=n))that has already been splitted\n\nStack trace returned 10 entries:\n[bt] (0) /usr/tvm/build/libtvm.so(dmlc::StackTrace[abi:cxx11]()+0x1bc) [0x7f8b6ea1618c]\n[bt] (1) /usr/tvm/build/libtvm.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f8b6ea16d68]\n[bt] (2) /usr/tvm/build/libtvm.so(+0x410148) [0x7f8b6ec39148]\n[bt] (3) /usr/tvm/build/libtvm.so(+0x4140b1) [0x7f8b6ec3d0b1]\n[bt] (4) /usr/tvm/build/libtvm.so(tvm::Stage::split(tvm::IterVar, HalideIR::Expr, tvm::IterVar*, tvm::IterVar*)+0x72) [0x7f8b6ec3d5e2]\n[bt] (5) /usr/tvm/build/libtvm.so(+0x1fe083) [0x7f8b6ea27083]\n[bt] (6) /usr/tvm/build/libtvm.so(TVMFuncCall+0x5e) [0x7f8b6ee237ee]\n[bt] (7) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call_unix64+0x4c) [0x7f8b9d1bfe20]\n[bt] (8) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call+0x2eb) [0x7f8b9d1bf88b]\n[bt] (9) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(_ctypes_callproc+0x49a) [0x7f8b9d1ba01a]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-84433a3fc19b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/tvm/python/tvm/schedule.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, parent, factor, nparts)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Either nparts or factor need to be provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0mouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_api_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_StageSplitByFactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/tvm/python/tvm/_ffi/function.py\u001b[0m in \u001b[0;36mmy_api_func\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mAPI\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \"\"\"\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mflocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmy_api_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/tvm/python/tvm/_ffi/_ctypes/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    182\u001b[0m         check_call(_LIB.TVMFuncCall(\n\u001b[1;32m    183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             ctypes.byref(ret_val), ctypes.byref(ret_tcode)))\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/tvm/python/tvm/_ffi/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \"\"\"\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTVMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTVMGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTVMError\u001b[0m: [07:03:18] /usr/tvm/src/schedule/schedule_lang.cc:30: Operate on iter var iter_var(i, Range(min=0, extent=n))that has already been splitted\n\nStack trace returned 10 entries:\n[bt] (0) /usr/tvm/build/libtvm.so(dmlc::StackTrace[abi:cxx11]()+0x1bc) [0x7f8b6ea1618c]\n[bt] (1) /usr/tvm/build/libtvm.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f8b6ea16d68]\n[bt] (2) /usr/tvm/build/libtvm.so(+0x410148) [0x7f8b6ec39148]\n[bt] (3) /usr/tvm/build/libtvm.so(+0x4140b1) [0x7f8b6ec3d0b1]\n[bt] (4) /usr/tvm/build/libtvm.so(tvm::Stage::split(tvm::IterVar, HalideIR::Expr, tvm::IterVar*, tvm::IterVar*)+0x72) [0x7f8b6ec3d5e2]\n[bt] (5) /usr/tvm/build/libtvm.so(+0x1fe083) [0x7f8b6ea27083]\n[bt] (6) /usr/tvm/build/libtvm.so(TVMFuncCall+0x5e) [0x7f8b6ee237ee]\n[bt] (7) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call_unix64+0x4c) [0x7f8b9d1bfe20]\n[bt] (8) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call+0x2eb) [0x7f8b9d1bf88b]\n[bt] (9) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(_ctypes_callproc+0x49a) [0x7f8b9d1ba01a]\n\n"
     ]
    }
   ],
   "source": [
    "bx, tx = s[C].split(C.op.axis[0], factor=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_var(i.outer, ) iter_var(i.inner, )\n"
     ]
    }
   ],
   "source": [
    "print (bx,tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we bind the iteration axis bx and tx to threads in the GPU\n",
    "compute grid. These are GPU specific constructs that allows us\n",
    "to generate code that runs on GPU.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tgt == \"cuda\":\n",
    "  s[C].bind(bx, tvm.thread_axis(\"blockIdx.x\"))\n",
    "  s[C].bind(tx, tvm.thread_axis(\"threadIdx.x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilation\n",
    "-----------\n",
    "After we have finished specifying the schedule, we can compile it\n",
    "into a TVM function. By default TVM compiles into a type-erased\n",
    "function that can be directly called from python side.\n",
    "\n",
    "In the following line, we use tvm.build to create a function.\n",
    "The build function takes the schedule, the desired signature of the\n",
    "function(including the inputs and outputs) as well as target language\n",
    "we want to compile to.\n",
    "\n",
    "The result of compilation fadd is a GPU device function(if GPU is involved)\n",
    "that can as well as a host wrapper that calls into the GPU function.\n",
    "fadd is the generated host wrapper function, it contains reference\n",
    "to the generated device function internally.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fadd = tvm.build(s, [A, B, C], tgt, target_host=tgt_host, name=\"myadd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Function\n",
    "----------------\n",
    "The compiled function TVM function is designed to be a concise C API\n",
    "that can be invoked from any languages.\n",
    "\n",
    "We provide an minimum array API in python to aid quick testing and prototyping.\n",
    "The array API is based on `DLPack <https://github.com/dmlc/dlpack>`_ standard.\n",
    "\n",
    "- We first create a gpu context.\n",
    "- Then tvm.nd.array copies the data to gpu.\n",
    "- fadd runs the actual computation.\n",
    "- asnumpy() copies the gpu array back to cpu and we can use this to verify correctness\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = tvm.context(tgt, 0)\n",
    "\n",
    "n = 1024\n",
    "a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), ctx)\n",
    "b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), ctx)\n",
    "c = tvm.nd.array(np.zeros(n, dtype=C.dtype), ctx)\n",
    "fadd(a, b, c)\n",
    "np.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the Generated Code\n",
    "--------------------------\n",
    "You can inspect the generated code in TVM. The result of tvm.build\n",
    "is a tvm Module. fadd is the host module that contains the host wrapper,\n",
    "it also contains a device module for the CUDA (GPU) function.\n",
    "\n",
    "The following code fetches the device module and prints the content code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----GPU code-----\n",
      "extern \"C\" __global__ void myadd_kernel0( float* __restrict__ C,  float* __restrict__ A,  float* __restrict__ B, int n) {\n",
      "  if (((int)blockIdx.x) < (n / 64)) {\n",
      "    C[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (A[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + B[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n",
      "  } else {\n",
      "    if ((((int)blockIdx.x) * 64) < (n - ((int)threadIdx.x))) {\n",
      "      C[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (A[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + B[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if tgt == \"cuda\":\n",
    "    dev_module = fadd.imported_modules[0]\n",
    "    print(\"-----GPU code-----\")\n",
    "    print(dev_module.get_source())\n",
    "else:\n",
    "    print(fadd.get_source())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Code Specialization\n",
    "\n",
    "  As you may noticed, during the declaration, A, B and C both\n",
    "  takes the same shape argument n. TVM will take advantage of this\n",
    "  to pass only single shape argument to the kernel, as you will find in\n",
    "  the printed device code. This is one form of specialization.\n",
    "\n",
    "  On the host side, TVM will automatically generate check code\n",
    "  that checks the constraints in the parameters. So if you pass\n",
    "  arrays with different shapes into the fadd, an error will be raised.\n",
    "\n",
    "  We can do more specializations. For example, we can write\n",
    "  :code:`n = tvm.convert(1024)` instead of :code:`n = tvm.var(\"n\")`,\n",
    "  in the computation declaration. The generated function will\n",
    "  only take vectors with length 1024.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Compiled Module\n",
    "--------------------\n",
    "Besides runtime compilation, we can save the compiled modules into\n",
    "file and load them back later. This is called ahead of time compilation.\n",
    "\n",
    "The following code first does the following step:\n",
    "\n",
    "- It saves the compiled host module into an object file.\n",
    "- Then it saves the device module into a ptx file.\n",
    "- cc.create_shared calls a env compiler(gcc) to create a shared library\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['myadd.o', 'myadd.tvm_meta.json', 'myadd.ptx', 'myadd.so']\n"
     ]
    }
   ],
   "source": [
    "from tvm.contrib import cc\n",
    "from tvm.contrib import util\n",
    "\n",
    "temp = util.tempdir()\n",
    "fadd.save(temp.relpath(\"myadd.o\"))\n",
    "if tgt == \"cuda\":\n",
    "    fadd.imported_modules[0].save(temp.relpath(\"myadd.ptx\"))\n",
    "cc.create_shared(temp.relpath(\"myadd.so\"), [temp.relpath(\"myadd.o\")])\n",
    "print(temp.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Module Storage Format\n",
    "\n",
    "  The CPU(host) module is directly saved as a shared library(so).\n",
    "  There can be multiple customed format on the device code.\n",
    "  In our example, device code is stored in ptx, as well as a meta\n",
    "  data json file. They can be loaded and linked seperatedly via import.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Compiled Module\n",
    "--------------------\n",
    "We can load the compiled module from the file system and run the code.\n",
    "The following code load the host and device module seperatedly and\n",
    "re-link them together. We can verify that the newly loaded function works.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fadd1 = tvm.module.load(temp.relpath(\"myadd.so\"))\n",
    "if tgt == \"cuda\":\n",
    "    fadd1_dev = tvm.module.load(temp.relpath(\"myadd.ptx\"))\n",
    "    fadd1.import_module(fadd1_dev)\n",
    "fadd1(a, b, c)\n",
    "np.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pack Everything into One Library\n",
    "--------------------------------\n",
    "In the above example, we store the device and host code seperatedly.\n",
    "TVM also supports export everything as one shared library.\n",
    "Under the hood, we pack the device modules into binary blobs and link\n",
    "them together with the host code.\n",
    "Currently we support packing of Metal, OpenCL and CUDA modules.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fadd.export_library(temp.relpath(\"myadd_pack.so\"))\n",
    "fadd2 = tvm.module.load(temp.relpath(\"myadd_pack.so\"))\n",
    "fadd2(a, b, c)\n",
    "np.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Runtime API and Thread-Safety\n",
    "\n",
    "  The compiled modules of TVM do not depend on the TVM compiler.\n",
    "  Instead, it only depends on a minimum runtime library.\n",
    "  TVM runtime library wraps the device drivers and provides\n",
    "  thread-safe and device agnostic call into the compiled functions.\n",
    "\n",
    "  This means you can call the compiled TVM function from any thread,\n",
    "  on any GPUs.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate OpenCL Code\n",
    "--------------------\n",
    "TVM provides code generation features into multiple backends,\n",
    "we can also generate OpenCL code or LLVM code that runs on CPU backends.\n",
    "\n",
    "The following codeblocks generate opencl code, creates array on opencl\n",
    "device, and verifies the correctness of the code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tgt == \"opencl\":\n",
    "    fadd_cl = tvm.build(s, [A, B, C], \"opencl\", name=\"myadd\")\n",
    "    print(\"------opencl code------\")\n",
    "    print(fadd_cl.imported_modules[0].get_source())\n",
    "    ctx = tvm.cl(0)\n",
    "    n = 1024\n",
    "    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), ctx)\n",
    "    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), ctx)\n",
    "    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), ctx)\n",
    "    fadd_cl(a, b, c)\n",
    "    np.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "-------\n",
    "This tutorial provides a walk through of TVM workflow using\n",
    "a vector add example. The general workflow is\n",
    "\n",
    "- Describe your computation via series of operations.\n",
    "- Describe how we want to compute use schedule primitives.\n",
    "- Compile to the target function we want.\n",
    "- Optionally, save the function to be loaded later.\n",
    "\n",
    "You are more than welcomed to checkout other examples and\n",
    "tutorials to learn more about the supported operations, schedule primitives\n",
    "and other features in TVM.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
