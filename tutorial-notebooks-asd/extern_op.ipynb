{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "External Tensor Functions\n",
    "=========================\n",
    "**Author**: `Tianqi Chen <https://tqchen.github.io>`_\n",
    "\n",
    "While TVM supports transparent code generation, sometimes\n",
    "it is also helpful to incorporate manual written code into\n",
    "the pipeline. For example, we might want to use cuDNN for\n",
    "some of the convolution kernels and define the rest of the stages.\n",
    "\n",
    "TVM supports these black box function calls natively.\n",
    "Specfically, tvm support all the tensor functions that are DLPack compatible.\n",
    "Which means we can call any function with POD types(pointer, int, float)\n",
    "or pointer to DLTensor as argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import tvm\n",
    "import numpy as np\n",
    "from tvm.contrib import cblas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Extern Tensor Function\n",
    "--------------------------\n",
    "In the example below, we use :any:`tvm.extern` to add an extern\n",
    "array function call. In the extern call, we declare the shape\n",
    "of output tensors. In the second argument we provide the list of inputs.\n",
    "\n",
    "User will need to provide a function describing how to compute the result.\n",
    "The compute function takes list of symbolic placeholder for the inputs,\n",
    "list of symbolic placeholder for the outputs and returns the executing statement.\n",
    "\n",
    "In this case we simply call a registered tvm function, which invokes a CBLAS call.\n",
    "TVM does not control internal of the extern array function and treats it as blackbox.\n",
    "We can further mix schedulable TVM calls that add a bias term to the result.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1024\n",
    "l = 128\n",
    "m = 235\n",
    "bias = tvm.var('bias', dtype=tvm.float32)\n",
    "A = tvm.placeholder((n, l), name='A')\n",
    "B = tvm.placeholder((l, m), name='B')\n",
    "C = tvm.extern((n, m), [A, B],\n",
    "               lambda ins, outs: tvm.call_packed(\n",
    "                   \"tvm.contrib.cblas.matmul\",\n",
    "                   ins[0], ins[1], outs[0], False, False), name=\"C\")\n",
    "D = tvm.compute(C.shape, lambda i, j: C[i,j] + bias, name=\"D\")\n",
    "s = tvm.create_schedule(D.op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the Result\n",
    "-----------------\n",
    "We can verify that the result matches what we expected.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = tvm.cpu(0)\n",
    "f = tvm.build(s, [A, B, D, bias], \"llvm\")\n",
    "a = tvm.nd.array(np.random.uniform(size=(n, l)).astype(A.dtype), ctx)\n",
    "b = tvm.nd.array(np.random.uniform(size=(l, m)).astype(B.dtype), ctx)\n",
    "d = tvm.nd.array(np.zeros((n, m), dtype=D.dtype), ctx)\n",
    "bb = 10.0\n",
    "f(a, b, d, bb)\n",
    "np.testing.assert_allclose(\n",
    "    d.asnumpy(), np.dot(a.asnumpy(), b.asnumpy()) + 10, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extern Contrib Wrappers\n",
    "-----------------------\n",
    "TVM also provide extern contrib wrappers to useful extern calls,\n",
    "the following line is equivalent to the previous example.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.contrib import cblas\n",
    "C = cblas.matmul(A, B)\n",
    "D = tvm.compute(C.shape, lambda i, j: C[i,j] + bias, name=\"D\")\n",
    "s = tvm.create_schedule(D.op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hook Python Function as Extern\n",
    "------------------------------\n",
    "Since we can call into any PackedFunc in TVM. We can use the extern\n",
    "function to callback into python.\n",
    "\n",
    "The following example registers a python function into tvm runtime system\n",
    "and use it to complete one stage of the computation.\n",
    "This makes TVM much more flexible. For example, we can insert front-end\n",
    "callbacks to inspect the intermediate results or mix customized code\n",
    "with TVM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_tvm_addone signatures: <class 'tvm.ndarray.NDArray'>, <class 'tvm.ndarray.NDArray'>\n"
     ]
    }
   ],
   "source": [
    "@tvm.register_func(\"tvm.contrib.my_tvm_addone\")\n",
    "def my_tvm_addone(x, y):\n",
    "    print(\"my_tvm_addone signatures: %s, %s\" % (type(x), type(y)))\n",
    "    tvm.nd.array(x.asnumpy() + 1).copyto(y)\n",
    "\n",
    "A = tvm.placeholder((n,), name='A')\n",
    "B = tvm.extern(A.shape, [A], lambda ins, outs: tvm.call_packed(\n",
    "    \"tvm.contrib.my_tvm_addone\", ins[0], outs[0]), name=\"C\")\n",
    "s = tvm.create_schedule(B.op)\n",
    "f = tvm.build(s, [A, B], \"llvm\")\n",
    "a = tvm.nd.array(np.random.uniform(size=(n,)).astype(A.dtype), ctx)\n",
    "b = tvm.nd.array(np.random.uniform(size=(n,)).astype(B.dtype), ctx)\n",
    "f(a, b)\n",
    "np.testing.assert_allclose(b.asnumpy(), a.asnumpy() + 1, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "-------\n",
    "- TVM calls extern tensor function via :any:`tvm.extern`\n",
    "- Use contrib wrappers for short sugars of extern tensor calls.\n",
    "- We can hook front-end function as extern tensor callbacks.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
